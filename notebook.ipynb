{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OrdinalEncoder,PolynomialFeatures, RobustScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_selection import SelectKBest, f_regression,SelectFromModel\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from striprtf.striprtf import rtf_to_text\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.ensemble import GradientBoostingClassifier,GradientBoostingRegressor\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression, Ridge, Lasso, ElasticNet\n",
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
    "from sklearn.svm import SVC,SVR\n",
    "from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor\n",
    "from sklearn.neural_network import MLPClassifier,MLPRegressor\n",
    "from sklearn.ensemble import ExtraTreesClassifier,ExtraTreesRegressor\n",
    "from sklearn.linear_model import SGDClassifier,SGDRegressor\n",
    "from xgboost import XGBClassifier, XGBRegressor\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper Functions\n",
    "def parse_json(json_data):\n",
    "    \"\"\"Parse JSON to extract relevant details.\"\"\"\n",
    "    target = json_data['design_state_data']['target']\n",
    "    feature_handling = json_data['design_state_data']['feature_handling']\n",
    "    feature_reduction = json_data['design_state_data']['feature_reduction']\n",
    "    algorithms = json_data['design_state_data']['algorithms']\n",
    "    return target, feature_handling, feature_reduction, algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    with open(r'E:\\New folder\\algoparams_from_ui.json.rtf', \"r\") as file:\n",
    "        rtf_content = file.read()\n",
    "        # Extract plain text from RTF\n",
    "        plain_text = rtf_to_text(rtf_content)\n",
    "        # Parse the JSON data\n",
    "        json_data = json.loads(plain_text)\n",
    "except json.JSONDecodeError as e:\n",
    "    print(f\"JSON Decode Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Errno 2] No such file or directory: 'iris_modified.csv'\n",
      "data readed\n"
     ]
    }
   ],
   "source": [
    "dataset_path = json_data['design_state_data']['session_info']['dataset']\n",
    "try:\n",
    "    df = pd.read_csv(dataset_path)\n",
    "    print('data readed')\n",
    "except FileNotFoundError as e:\n",
    "    print(e)\n",
    "    try:\n",
    "        path=input('plz give the path of dataset')\n",
    "        df=pd.read_csv(rf'{path}')\n",
    "        print('data readed')\n",
    "    except Exception as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_json(json_data):\n",
    "    \"\"\"Parse JSON to extract relevant details.\"\"\"\n",
    "    target = json_data['design_state_data']['target']\n",
    "    train=json_data['design_state_data']['train']\n",
    "    feature_handling = json_data['design_state_data']['feature_handling']\n",
    "    feature_generation=json_data['design_state_data']['feature_generation']\n",
    "    feature_reduction = json_data['design_state_data']['feature_reduction']\n",
    "    algorithms = json_data['design_state_data']['algorithms']\n",
    "\n",
    "    return target,train, feature_handling,feature_generation, feature_reduction, algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "target,train, feature_handling,feature_generation, feature_reduction, algorithms=parse_json(json_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test(X, y,config):\n",
    "    \n",
    "    # Extract configuration values\n",
    "    policy = config.get(\"policy\", \"Split the dataset\")\n",
    "    time_variable = config.get(\"time_variable\", None)\n",
    "    sampling_method = config.get(\"sampling_method\", \"No sampling(whole data)\")\n",
    "    split = config.get(\"split\", \"Randomly\")\n",
    "    k_fold = config.get(\"k_fold\", False)\n",
    "    train_ratio = config.get(\"train_ratio\", 0.8)\n",
    "    random_seed = config.get(\"random_seed\", 0)\n",
    "    \n",
    "    if policy != \"Split the dataset\":\n",
    "        raise ValueError(\"Only 'Split the dataset' policy is supported.\")\n",
    "    \n",
    "    if sampling_method != \"No sampling(whole data)\":\n",
    "        raise ValueError(\"Only 'No sampling(whole data)' is supported.\")\n",
    "    \n",
    "    if k_fold:\n",
    "        kf = KFold(n_splits=5, shuffle=(split == \"Randomly\"), random_state=random_seed)\n",
    "        accuracies = []\n",
    "        \n",
    "        for train_idx, test_idx in kf.split(X):\n",
    "            X_train, X_test = X[train_idx], X[test_idx]\n",
    "            y_train, y_test = y[train_idx], y[test_idx]\n",
    "            \n",
    "        \n",
    "        return X_train, X_test, y_train, y_test\n",
    "    else:\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X, y, test_size=1-train_ratio, random_state=random_seed, shuffle=(split == \"Randomly\")\n",
    "        )\n",
    "        \n",
    "        return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputer=[]\n",
    "col_n=[]\n",
    "for feature, details in feature_handling.items():\n",
    "    if details['is_selected']:\n",
    "        col_n.append(feature)\n",
    "        if details['feature_variable_type'] == 'numerical':\n",
    "            if details['feature_details']['missing_values'] == 'Impute':\n",
    "                strategy = 'mean' if details['feature_details']['impute_with'] == 'Average of values' else 'constant'\n",
    "                fill_value = details['feature_details'].get('impute_value', 0)\n",
    "                imputer.append((f'{feature}',SimpleImputer(strategy=strategy, fill_value=fill_value),[col_n.index(feature)]))\n",
    "        if details['feature_variable_type'] == \"text\":\n",
    "            if details['feature_details'][\"text_handling\"] == \"Tokenize and hash\":\n",
    "                imputer.append((f'{feature}',OrdinalEncoder(),[col_n.index(feature)]))\n",
    "df[col_n],\n",
    "trs=ColumnTransformer(imputer,remainder='passthrough',n_jobs=-1)\n",
    "\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.preprocessing import RobustScaler, StandardScaler\n",
    "\n",
    "class FeatureGenerationTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, config, feature_names=None):\n",
    "        self.config = config\n",
    "        self.feature_names = feature_names\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def _generate_interactions(self, X):\n",
    "\n",
    "        # Linear Interactions\n",
    "        for f1, f2 in self.config.get(\"linear_interactions\", []):\n",
    "            interaction = X[f1] * X[f2]\n",
    "            if self.config.get(\"linear_scalar_type\") == \"robust\":\n",
    "                scaler = RobustScaler()\n",
    "                interaction = scaler.fit_transform(interaction.values.reshape(-1, 1)).flatten()\n",
    "            X['linear_interactions']=interaction\n",
    "\n",
    "        # Polynomial Interactions (Ratios)\n",
    "        def interaction_terms(X):\n",
    "            X = np.asarray(X)  # Ensure input is a NumPy array\n",
    "            return X[:, 2] / X[:, 1]*X[:, 3] / (X[:, 4] + 1e-8) # petal_length / sepal_width\n",
    "                  # petal_width / species\n",
    "        if 'polynomial_interactions' in feature_generation.keys():\n",
    "            X['polynomial_interactions']= interaction_terms(X)\n",
    "\n",
    "        # Explicit Pairwise Interactions (Ratios)\n",
    "        def pairwise_interactions(X):\n",
    "            X = np.asarray(X)  # Ensure input is a NumPy array\n",
    "            return X[:, 1] / X[:, 0]* X[:, 3] / X[:, 0]  # sepal_width / sepal_length\n",
    "                                                        # petal_width / sepal_length\n",
    "            \n",
    "        if 'explicit_pairwise_interactions' in feature_generation.keys():\n",
    "            X['pairwise_interactions']= pairwise_interactions(X)\n",
    "        \n",
    "        return X\n",
    "\n",
    "    def transform(self, X):\n",
    "        X=pd.DataFrame(X,columns=col_n)\n",
    "        X_transformed = self._generate_interactions(X)\n",
    "        return X_transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['linear_interactions', 'linear_scalar_type', 'polynomial_interactions', 'explicit_pairwise_interactions'])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_generation.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature generation\n",
    "feature_gen = json_data['design_state_data']['feature_generation']\n",
    "fg=('feature_generation', FeatureGenerationTransformer(feature_gen, col_n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_feature_reduction(feature_reduction):\n",
    "    \"\"\"Reduce features based on the specified method.\"\"\"\n",
    "    method = feature_reduction['feature_reduction_method']\n",
    "        \n",
    "    if method == 'Corr with Target':\n",
    "        return ('feature_selection',SelectKBest(score_func=f_regression, k=int(feature_reduction['num_of_features_to_keep'])))\n",
    "    elif method == 'Tree-based':\n",
    "        if target['prediction_type']=='Regression':\n",
    "            return ('feature_selection', SelectFromModel(RandomForestRegressor(n_estimators=int(feature_reduction['num_of_trees']),\n",
    "                                                                        max_depth=int(feature_reduction['depth_of_trees'])),\n",
    "                                                                        max_features=int(feature_reduction['num_of_features_to_keep']),\n",
    "                                                                        threshold=-np.inf))\n",
    "        if target['prediction_type']=='Classifier':\n",
    "            return ('feature_selection', SelectFromModel(RandomForestClassifier(n_estimators=int(feature_reduction['num_of_trees']),\n",
    "                                                                        max_depth=int(feature_reduction['depth_of_trees'])),\n",
    "                                                                        max_features=int(feature_reduction['num_of_features_to_keep']),\n",
    "                                                                        threshold=-np.inf))\n",
    "    elif method == 'PCA':\n",
    "        return ('feature_selection', PCA(n_components=int(feature_reduction['num_of_features_to_keep'])))\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_red=apply_feature_reduction(feature_reduction)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "D_Regressor = {\n",
    "    'RandomForestRegressor': 'RandomForestRegressor',\n",
    "    'GBTRegressor': 'GradientBoostingRegressor',\n",
    "    'LinearRegression': 'LinearRegression',\n",
    "    'LogisticRegression': 'LogisticRegression',\n",
    "    'RidgeRegression': 'Ridge',\n",
    "    'LassoRegression': 'Lasso',\n",
    "    'ElasticNetRegression': 'ElasticNet',\n",
    "    'xg_boost': 'XGBRegressor', \n",
    "    'DecisionTreeRegressor': 'DecisionTreeRegressor',\n",
    "    'SVM': 'SVR',\n",
    "    'SGD': 'SGDRegressor',\n",
    "    'KNN': 'KNeighborsRegressor',\n",
    "    'extra_random_trees': 'ExtraTreesRegressor',\n",
    "    'neural_network': 'MLPRegressor',\n",
    "}\n",
    "\n",
    "D_Classifier = {\n",
    "    'RandomForestClassifier': 'RandomForestClassifier',\n",
    "    'GBTClassifier': 'GradientBoostingClassifier',\n",
    "    'LogisticRegression': 'LogisticRegression',\n",
    "    'xg_boost': 'XGBClassifier',  \n",
    "    'DecisionTreeClassifier': 'DecisionTreeClassifier',\n",
    "    'SVM': 'SVC',\n",
    "    'SGD': 'SGDClassifier',\n",
    "    'KNN': 'KNeighborsClassifier',\n",
    "    'extra_random_trees': 'ExtraTreesClassifier',\n",
    "    'neural_network': 'MLPClassifier',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Get selected models for the prediction type.\"\"\"\n",
    "selected_models = []\n",
    "prediction_type=target['prediction_type']\n",
    "for algo_name, algo_details in algorithms.items():\n",
    "    if algo_details['is_selected']:\n",
    "        if prediction_type.lower() == 'regression':\n",
    "            if algo_name in D_Regressor.keys():\n",
    "                selected_models.append(Pipeline(steps=[feature_red,(f'{algo_name}',eval(f'{D_Regressor[algo_name]}()'))]))\n",
    "\n",
    "        elif prediction_type.lower() == 'classifier':\n",
    "            selected_models.append(Pipeline(steps=[feature_red,(f'{algo_name}',eval(f'{D_Classifier[algo_name]}()'))]))\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'KNeighborsRegressor'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "D_Regressor['KNN']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_param_grid(algorithms):\n",
    "    param_grid = {}\n",
    "    for algo_name, algo_details in algorithms.items():\n",
    "        if algo_details[\"is_selected\"]:\n",
    "            grid = {}\n",
    "\n",
    "            # Random Forest parameters\n",
    "            if algo_name in [\"RandomForestClassifier\", \"RandomForestRegressor\", \"DecisionTreeClassifier\", \"DecisionTreeRegressor\"]:\n",
    "                if \"min_trees\" in algo_details and \"max_trees\" in algo_details:\n",
    "                    grid[\"n_estimators\"] = list(range(algo_details[\"min_trees\"], algo_details[\"max_trees\"] + 1))\n",
    "                if \"min_depth\" in algo_details and \"max_depth\" in algo_details:\n",
    "                    grid[\"max_depth\"] = list(range(algo_details[\"min_depth\"], algo_details[\"max_depth\"] + 1))\n",
    "                if \"min_samples_per_leaf_min_value\" in algo_details and \"min_samples_per_leaf_max_value\" in algo_details:\n",
    "                    grid[\"min_samples_leaf\"] = list(range(algo_details[\"min_samples_per_leaf_min_value\"], algo_details[\"min_samples_per_leaf_max_value\"] + 1))\n",
    "                if \"min_samples_per_leaf\" in algo_details:\n",
    "                    grid[\"min_samples_leaf\"] = algo_details[\"min_samples_per_leaf\"]\n",
    "\n",
    "            # Gradient Boosted Trees parameters\n",
    "            if algo_name in [\"GBTClassifier\", \"GBTRegressor\"]:\n",
    "                if \"num_of_BoostingStages\" in algo_details:\n",
    "                    grid[\"n_estimators\"] = algo_details[\"num_of_BoostingStages\"]\n",
    "                if \"min_stepsize\" in algo_details and \"max_stepsize\" in algo_details:\n",
    "                    grid[\"learning_rate\"] = [x / 100.0 for x in range(int(algo_details[\"min_stepsize\"] * 100), int(algo_details[\"max_stepsize\"] * 100) + 1)]\n",
    "                if \"min_depth\" in algo_details and \"max_depth\" in algo_details:\n",
    "                    grid[\"max_depth\"] = list(range(algo_details[\"min_depth\"], algo_details[\"max_depth\"] + 1))\n",
    "\n",
    "            # Linear and Logistic Regression parameters\n",
    "            if algo_name in [\"LinearRegression\", \"LogisticRegression\", \"RidgeRegression\", \"LassoRegression\", \"ElasticNetRegression\"]:\n",
    "                if \"min_iter\" in algo_details and \"max_iter\" in algo_details:\n",
    "                    grid[\"max_iter\"] = list(range(algo_details[\"min_iter\"], algo_details[\"max_iter\"] + 1))\n",
    "                if \"min_regparam\" in algo_details and \"max_regparam\" in algo_details:\n",
    "                    grid[\"alpha\"] = [x / 100.0 for x in range(int(algo_details[\"min_regparam\"] * 100), int(algo_details[\"max_regparam\"] * 100) + 1)]\n",
    "                if \"min_elasticnet\" in algo_details and \"max_elasticnet\" in algo_details:\n",
    "                    grid[\"l1_ratio\"] = [x / 100.0 for x in range(int(algo_details[\"min_elasticnet\"] * 100), int(algo_details[\"max_elasticnet\"] * 100) + 1)]\n",
    "\n",
    "            # XGBoost parameters\n",
    "            if algo_name == \"xg_boost\":\n",
    "                if \"max_depth_of_tree\" in algo_details:\n",
    "                    grid[\"max_depth\"] = algo_details[\"max_depth_of_tree\"]\n",
    "                if \"learningRate\" in algo_details:\n",
    "                    grid[\"learning_rate\"] = algo_details[\"learningRate\"]\n",
    "                if \"l1_regularization\" in algo_details:\n",
    "                    grid[\"reg_alpha\"] = algo_details[\"l1_regularization\"]\n",
    "                if \"l2_regularization\" in algo_details:\n",
    "                    grid[\"reg_lambda\"] = algo_details[\"l2_regularization\"]\n",
    "                if \"gamma\" in algo_details:\n",
    "                    grid[\"gamma\"] = algo_details[\"gamma\"]\n",
    "                if \"min_child_weight\" in algo_details:\n",
    "                    grid[\"min_child_weight\"] = algo_details[\"min_child_weight\"]\n",
    "                if \"sub_sample\" in algo_details:\n",
    "                    grid[\"subsample\"] = algo_details[\"sub_sample\"]\n",
    "                if \"col_sample_by_tree\" in algo_details:\n",
    "                    grid[\"colsample_bytree\"] = algo_details[\"col_sample_by_tree\"]\n",
    "\n",
    "            # SVM parameters\n",
    "            if algo_name == \"SVM\":\n",
    "                if \"c_value\" in algo_details:\n",
    "                    grid[\"C\"] = algo_details[\"c_value\"]\n",
    "                if \"custom_gamma_values\" in algo_details and algo_details[\"custom_gamma_values\"]:\n",
    "                    grid[\"gamma\"] = [\"scale\", \"auto\"]  # Add custom gamma values if needed\n",
    "\n",
    "            # KNN parameters\n",
    "            if algo_name == \"KNN\":\n",
    "                if \"k_value\" in algo_details:\n",
    "                    grid[\"n_neighbors\"] = algo_details[\"k_value\"]\n",
    "                if \"p_value\" in algo_details:\n",
    "                    grid[\"p\"] = [algo_details[\"p_value\"]]\n",
    "\n",
    "            # Extra Random Trees parameters\n",
    "            if algo_name == \"extra_random_trees\":\n",
    "                if \"num_of_trees\" in algo_details:\n",
    "                    grid[\"n_estimators\"] = algo_details[\"num_of_trees\"]\n",
    "                if \"max_depth\" in algo_details:\n",
    "                    grid[\"max_depth\"] = algo_details[\"max_depth\"]\n",
    "                if \"min_samples_per_leaf\" in algo_details:\n",
    "                    grid[\"min_samples_leaf\"] = algo_details[\"min_samples_per_leaf\"]\n",
    "\n",
    "            # Neural Network parameters\n",
    "            if algo_name == \"neural_network\":\n",
    "                if \"hidden_layer_sizes\" in algo_details:\n",
    "                    grid[\"hidden_layer_sizes\"] = algo_details[\"hidden_layer_sizes\"]\n",
    "                if \"activation\" in algo_details and algo_details[\"activation\"]:\n",
    "                    grid[\"activation\"] = [algo_details[\"activation\"]]\n",
    "                if \"solver\" in algo_details:\n",
    "                    grid[\"solver\"] = [algo_details[\"solver\"]]\n",
    "                if \"max_iterations\" in algo_details and algo_details[\"max_iterations\"] > 0:\n",
    "                    grid[\"max_iter\"] = [algo_details[\"max_iterations\"]]\n",
    "\n",
    "            # Add grid to parameter dictionary if non-empty\n",
    "        \n",
    "            if grid:\n",
    "                param_grid[algo_name] = {f'{algo_name}__{key}':value for key, value in grid.items()}\n",
    "\n",
    "    \n",
    "\n",
    "    return param_grid\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Define pipeline\n",
    "pipeline1 = Pipeline(steps=[\n",
    "        ('preprocessor', trs),\n",
    "        fg,\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pipeline1.fit_transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test=train_test(X=df.drop(target['target'],axis=1),y=df[[target['target']]],config=train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_general_metrics(y_true, y_pred, task_type, \n",
    "                            compute_lift_at=0, \n",
    "                            cost_matrix=None,\n",
    "                            optimize_model_hyperparameters_for=\"AUC\",\n",
    "                            optimize_threshold_for=\"F1 Score\"):\n",
    "   \n",
    "    metrics_dict = {}\n",
    "    \n",
    "    # Default cost matrix values\n",
    "    if cost_matrix is None:\n",
    "        cost_matrix = {\n",
    "            \"gain_tp\": 1,  # True Positive\n",
    "            \"gain_fp\": 0,  # False Positive\n",
    "            \"gain_fn\": 0,  # False Negative\n",
    "            \"gain_tn\": 0   # True Negative\n",
    "        }\n",
    "\n",
    "    # Classification metrics\n",
    "    if task_type == \"classification\":\n",
    "        # Check if predictions are probabilities\n",
    "        if len(np.unique(y_pred)) > 2 and np.min(y_pred) >= 0 and np.max(y_pred) <= 1:\n",
    "            y_pred_labels = (y_pred >= 0.5).astype(int)  # Default threshold = 0.5\n",
    "        else:\n",
    "            y_pred_labels = y_pred\n",
    "        \n",
    "        # Compute basic metrics\n",
    "        if optimize_model_hyperparameters_for == \"AUC\":\n",
    "            metrics_dict[\"AUC\"] = roc_auc_score(y_true, y_pred)\n",
    "        \n",
    "        if optimize_threshold_for == \"F1 Score\":\n",
    "            metrics_dict[\"F1 Score\"] = f1_score(y_true, y_pred_labels)\n",
    "        \n",
    "        # Compute custom cost matrix gain\n",
    "        gain = 0\n",
    "        for yt, yp in zip(y_true, y_pred_labels):\n",
    "            if yt == 1 and yp == 1:\n",
    "                gain += cost_matrix[\"gain_tp\"]\n",
    "            elif yt == 0 and yp == 1:\n",
    "                gain += cost_matrix[\"gain_fp\"]\n",
    "            elif yt == 1 and yp == 0:\n",
    "                gain += cost_matrix[\"gain_fn\"]\n",
    "            elif yt == 0 and yp == 0:\n",
    "                gain += cost_matrix[\"gain_tn\"]\n",
    "        metrics_dict[\"Custom Gain\"] = gain\n",
    "\n",
    "        # Compute lift if required\n",
    "        if compute_lift_at > 0:\n",
    "            sorted_predictions = sorted(zip(y_true, y_pred), key=lambda x: x[1], reverse=True)\n",
    "            top_n = int(len(sorted_predictions) * compute_lift_at)\n",
    "            y_true_top_n = [yt for yt, _ in sorted_predictions[:top_n]]\n",
    "            lift_gain = sum(y_true_top_n) * cost_matrix[\"gain_tp\"]\n",
    "            metrics_dict[\"Lift at {:.0%}\".format(compute_lift_at)] = lift_gain\n",
    "\n",
    "    # Regression metrics\n",
    "    elif task_type == \"regression\":\n",
    "        metrics_dict[\"MSE\"] = mean_squared_error(y_true, y_pred)\n",
    "        metrics_dict[\"MAE\"] = mean_absolute_error(y_true, y_pred)\n",
    "         # Suppress the specific warning\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.simplefilter(\"ignore\", UndefinedMetricWarning)\n",
    "            metrics_dict[\"R2\"] = r2_score(y_true, y_pred)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"Invalid task_type. Must be 'classification' or 'regression'.\")\n",
    "    \n",
    "    return metrics_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import TimeSeriesSplit, GridSearchCV\n",
    "from sklearn.metrics import make_scorer, roc_auc_score, f1_score\n",
    "\n",
    "def custom_grid_search_cv(estimator_list, param_grid, X, y,task_type):\n",
    "   \n",
    "    # Time-based K-Fold (with overlap)\n",
    "    tscv = TimeSeriesSplit(n_splits=6)  # TimeSeriesSplit doesn't support stratification\n",
    "\n",
    "    # Custom scoring metrics\n",
    "    scoring = 'r2' if target['prediction_type'].lower()=='regression' else 'accuracy'\n",
    "    for estimator,i in zip(estimator_list,param_grid):\n",
    "        # Initialize GridSearchCV with the given parameters\n",
    "        grid_search = GridSearchCV(\n",
    "            estimator = estimator,\n",
    "            param_grid=param_grid[i],\n",
    "            scoring=scoring,\n",
    "            cv=tscv,\n",
    "            n_jobs=5,  # Parallelism\n",
    "            verbose=1\n",
    "        )\n",
    "        # Fit the grid search\n",
    "        print(i)\n",
    "        grid_search.fit(X, y.values.ravel())\n",
    "        \n",
    "        metrics_dict=compute_general_metrics(y,grid_search.predict(X),task_type)\n",
    "        print(metrics_dict)\n",
    "        return grid_search.best_estimator_,grid_search.best_score_, grid_search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestRegressor\n",
      "Fitting 6 folds for each of 396 candidates, totalling 2376 fits\n",
      "{'MSE': 0.0092338214737067, 'MAE': 0.06666660472765498, 'R2': 0.9838756561396161}\n"
     ]
    }
   ],
   "source": [
    "a=custom_grid_search_cv(estimator_list=selected_models,param_grid=param_grid, X=X_train, y=y_train,task_type='regression')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'MSE': 0.005852967456984862,\n",
       " 'MAE': 0.049272989128693837,\n",
       " 'R2': 0.9897793930553245}"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_general_metrics(y_true=y_train,y_pred=a.predict(X_train),task_type='regression')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from sklearn.exceptions import UndefinedMetricWarning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "testenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
